{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/istRsapp/Anaconda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', 'did', 'didn', 'do', 'does', 'doesn', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'has', 'hasn', 'have', 'haven', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'if', 'in', 'into', 'is', 'isn', 'it', 'its', 'itself', 'just', 'll', 'ma', 'me', 'mightn', 'more', 'most', 'mustn', 'my', 'myself', 'needn', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 'same', 'shan', 'she', 'should', 'shouldn', 'so', 'some', 'such', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', 'we', 'were', 'weren', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', 'wouldn', 'you', 'your', 'yours', 'yourself', 'yourselves'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/opt/local/istRsapp/Anaconda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import pickle # for saving and loading objects\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from nltk.tag import pos_tag\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import tarfile\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import OrderedDict\n",
    "\n",
    "#Extracting an open source dataset from the New York Times\n",
    "tar = tarfile.open(\"20news-bydate.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "##Make a list of the folders in the dataset\n",
    "directory = [f for f in os.listdir('./20news-bydate-train') if not f.startswith('.')]\n",
    "dt_stamp = datetime.datetime.strftime(datetime.datetime.now(),'%Y%m%d_%H_%M_%S')\n",
    "\n",
    "texts=[]\n",
    "text_corpus=[]\n",
    "texts_mod=[]\n",
    "\n",
    "for i in range(len(directory)):\n",
    "##Create a list of files in the given dictionary \n",
    "    files = os.listdir('./20news-bydate-train/' + directory[i])\n",
    "    \n",
    "    for j in range(len(files)):     \n",
    "\n",
    "        ##Path of each file \n",
    "        path = './20news-bydate-train/' + directory[i] + '/' + files[j]\n",
    "\n",
    "        ##open the file and read it\n",
    "        text1 = open(path, 'r', errors='ignore').read()        \n",
    "        texts.append(text1)\n",
    "\n",
    "# Data Cleansing\n",
    "for text1 in texts:\n",
    "    text = text1\n",
    "    #remove_names()\n",
    "    new_sentence = []\n",
    "    text_split=text.split(\" \")\n",
    "    tagged_sentence = pos_tag([word for word in text_split if word])\n",
    "    for word, tag in tagged_sentence:\n",
    "        if tag in ['NNP', 'NNPS']:\n",
    "            lemma_word = \"\"\n",
    "        else:\n",
    "            lemma_word = word\n",
    "\n",
    "        new_sentence.append(lemma_word)\n",
    "    text2=\"\"\n",
    "    for i in new_sentence:\n",
    "        text2 = text2 + \" \" + i\n",
    "    text=text2\n",
    "\n",
    "    # Converting to Lower case\n",
    "    text=text.lower()\n",
    "\n",
    "    # Removing unwanted information \n",
    "    text=text.replace(\"-\",\" \").replace(\".com\",\" \")\n",
    "    text=' '.join(re.sub(\"(@[A-Za-z[0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\/S+)\", \" \", text).split())\n",
    "\n",
    "    # Adding Space at begining and End of the Text\n",
    "    text=\" \" + text + \" \"\n",
    "\n",
    "    #replace_incomplete_word()\n",
    "\n",
    "    #smart_lemmatize()\n",
    "    new_sentence = []\n",
    "    lemma = WordNetLemmatizer()\n",
    "    text_split=text.split(\" \")\n",
    "    tagged_sentence = pos_tag([word for word in text_split if word])\n",
    "    for word, tag in tagged_sentence:\n",
    "        if tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            pos = 'n'\n",
    "        elif tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "            pos = 'v'\n",
    "        elif tag in ['JJ', 'JJR', 'JJS']:\n",
    "            pos = 'a'\n",
    "        elif tag in ['RB', 'RBR', 'RBS']:\n",
    "            pos = 'r'\n",
    "        else:\n",
    "            pos = 'n'\n",
    "        lemma_word = lemma.lemmatize(word=word, pos=pos)\n",
    "        new_sentence.append(lemma_word)\n",
    "    text2=\"\"\n",
    "    for i in new_sentence:\n",
    "        text2 = text2 + \" \" + i\n",
    "    text=text2\n",
    "\n",
    "    #removeUnicode()\n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r' ', text)       \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "\n",
    "    #replaceURL()\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\n",
    "    #replaceAtUser()\n",
    "    text = re.sub('@[^\\s]+',' ',text)\n",
    "\n",
    "    #removeHashtagInFrontOfWord()\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\n",
    "    #removeNumbers()\n",
    "    text = ''.join([i for i in text if not i.isdigit()]) \n",
    "\n",
    "    #replaceMultiExclamationMark()\n",
    "    text = re.sub(r\"(\\!)\\1+\", ' ', text)\n",
    "\n",
    "    #replaceMultiQuestionMark()\n",
    "    text = re.sub(r\"(\\?)\\1+\", ' ', text)\n",
    "\n",
    "    #replaceMultiStopMark()\n",
    "    text = re.sub(r\"(\\.)\\1+\", '.', text)\n",
    "\n",
    "    #replace_incomplete_word()\n",
    "    #remove_stop_words()\n",
    "    from nltk.corpus import stopwords\n",
    "    rmv_wrd_lst=stopwords.words('english')\n",
    "    rmv_stop_word=[]\n",
    "    for i in rmv_wrd_lst:\n",
    "        #print(i)\n",
    "        wrd=\" \" + str(i) + \" \"\n",
    "        rmv_stop_word.append(wrd)\n",
    "    for t in rmv_stop_word:\n",
    "        text=text.replace(t,\" \")\n",
    "\n",
    "    #replaceMultiSpace()\n",
    "    #text=text.replace(\"  \",\" \")\n",
    "    text = re.sub(r\"(\\ )\\1+\", ' ', text)        \n",
    "\n",
    "    text_corpus.append(gensim.utils.simple_preprocess(str((text.split()))))\n",
    "    #texts.append(gensim.utils.simple_preprocess(str((clean_text_ngram(text1)).split()),deacc=True))\n",
    "    texts_mod.append(text)\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word',stop_words=rmv_stop_word)\n",
    "data_vectorized = vectorizer.fit_transform([' '.join(x) for x in text_corpus])\n",
    "search_params = {'n_components': [1, 2, 3, 4, 5, 10, 15, 20]}\n",
    "lda = LatentDirichletAllocation()\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "model.fit(data_vectorized)\n",
    "print(\"best model params\", model.best_params_)\n",
    "\n",
    "# Cluster Number optimization using LDA Coherence Score\n",
    "dictionary=corpora.Dictionary(text_corpus)\n",
    "corpus=[dictionary.doc2bow(text) for text in text_corpus]\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "i_model_list = []\n",
    "i_cv_list = []\n",
    "limit=21\n",
    "start=2\n",
    "step=1\n",
    "\n",
    "for num_topics in range(start, limit, step):\n",
    "            print('Number of Topics', num_topics)\n",
    "            model = gensim.models.LdaModel(corpus,num_topics=num_topics, id2word=dictionary,random_state=100,chunksize=10000,passes=2,alpha='auto')\n",
    "            \n",
    "            coherencemodel = CoherenceModel(model=model, texts=text_corpus,corpus=corpus,dictionary=dictionary, coherence='c_v')\n",
    "            print(' CV ' , coherencemodel.get_coherence())\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "            model_list.append(num_topics)\n",
    "            \n",
    "            \n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "limit=21\n",
    "start=2\n",
    "step=1\n",
    "x=range(start,limit,step)\n",
    "plt.plot(x,coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend(\"Coherence_values\",loc='best')\n",
    "plt.grid()\n",
    "work_dir=os.getcwd()\n",
    "title_name=\"20_newsgroup_plot\"\n",
    "img_name=str(work_dir)+'/'+str(title_name)+ str(\".png\")\n",
    "#fig=plt.figure(figsize=(6,4))\n",
    "plt.savefig(img_name,dpi=300,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# Final LDA Model with optimized number of clusters as identified in the above coherence plot\n",
    "lda_model =  gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=dictionary,num_topics=5,update_every=1,chunksize=10000,passes=1,random_state=100)\n",
    "print(lda_model.print_topics(12))\n",
    "\n",
    "# Saving the Final LDA Model and Data as Pickle file\n",
    "with open ('text_corpus_20Newgroup_' + dt_stamp + 'pkl', 'wb') as pkl_fl:\n",
    "        pickle.dump(text_corpus,pkl_fl)\n",
    "        \n",
    "with open ('lda_model_20Newgroup_' + dt_stamp + 'pkl', 'wb') as pkl_fl:\n",
    "        pickle.dump(lda_model,pkl_fl)\n",
    "        \n",
    "# LDA Visualization\n",
    "pyLDAvis.enable_notebook()\n",
    "viz=pyLDAvis.gensim.prepare(lda_model,corpus,dictionary,sort_topics=False)\n",
    "pyLDAvis.display(viz)\n",
    "\n",
    "# save the LDA visualization\n",
    "\n",
    "dt_stamp = datetime.datetime.strftime(datetime.datetime.now(),'%Y%m%d_%H_%M_%S')\n",
    "pyLDAvis.save_html(viz,'lda_visualization_20Newgroup_' + dt_stamp + '.html')\n",
    "\n",
    "# # LDA Output\n",
    "freq_df = viz.topic_info\n",
    "freq_df['prevalence'] = freq_df['Freq'] / freq_df['Total'] # calculates prevalence ratio (0 â€“ 1)\n",
    "freq_df = freq_df.loc[freq_df['Category'] != 'Default'] # filters out extra data\n",
    "freq_df.to_excel('20Newgroup_keywords_' + dt_stamp + '.xlsx') # exports to excel\n",
    "\n",
    "sent_topics_df = pd.DataFrame()\n",
    "for i,row_list in enumerate(lda_model[corpus]):\n",
    "    row = row_list[0] if lda_model.per_word_topics else row_list  # get list of sentences\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "    for j , (topic_num,prop_topic) in enumerate(row):\n",
    "        if j == 0:\n",
    "            wp =lda_model.show_topic(topic_num)\n",
    "            topic_keywords = \", \".join([word for word,prop in wp])\n",
    "            sent_topics_df=sent_topics_df.append(pd.Series([int(topic_num),round(prop_topic,4),topic_keywords]),ignore_index=True)\n",
    "        else:\n",
    "            break\n",
    "sent_topics_df.columns = ['Dominant_Topic','Perc_Contribution','Topic_Keywords']\n",
    "contents = pd.Series(texts_mod)\n",
    "contents_act = pd.Series(texts)\n",
    "sent_topics_df = pd.concat([sent_topics_df,contents_act,contents],axis =1)\n",
    "\n",
    "df_dominant_topic = sent_topics_df.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No' ,'Dominant_Topic','Topic_Perc_Contrib','keywords','Original Text','Clean Text']\n",
    "df_dominant_topic.to_excel('20Newgroup_topic_by_document_' + dt_stamp + '.xlsx')\n",
    "df_dominant_topic.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
